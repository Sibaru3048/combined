# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UCIQLvjQa_lRPOfhW1gHDSaKCq2Y0Lbw
"""

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from transformers import PegasusTokenizer, PegasusForConditionalGeneration
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import re

app = FastAPI()

# 요약 모델 및 토크나이저 설정
summarizer_model_name = "EXP442/pegasus_summarizer"
summarizer_tokenizer = PegasusTokenizer.from_pretrained(summarizer_model_name)
summarizer_model = PegasusForConditionalGeneration.from_pretrained(summarizer_model_name)

# 번역 모델 및 토크나이저 설정
translator_model_name = "facebook/nllb-200-distilled-600M"
translator_tokenizer = AutoTokenizer.from_pretrained(translator_model_name)
translator_model = AutoModelForSeq2SeqLM.from_pretrained(translator_model_name)

class TextInput(BaseModel):
    text: str

def split_text_with_last_sentence_overlap(text, target_chunk_length=2048):
    # 문장 단위로 텍스트 분할
    sentences = re.split(r'(?<=[.!?]) +', text)
    chunks = []
    current_chunk = ""

    for sentence in sentences:
        if len(current_chunk) + len(sentence) <= target_chunk_length:
            current_chunk += sentence + " "
        else:
            chunks.append(current_chunk.strip())
            current_chunk = chunks[-1].split()[-1] + " " + sentence + " "

    if current_chunk:
        chunks.append(current_chunk.strip())

    return chunks

def summarize_long_text(article_text, target_chunk_length=2048):
    # 텍스트를 분할하여 요약
    chunks = split_text_with_last_sentence_overlap(article_text, target_chunk_length)
    summaries = []

    for chunk in chunks:
        inputs = summarizer_tokenizer(chunk, max_length=target_chunk_length, return_tensors="pt", truncation=True)
        summary_ids = summarizer_model.generate(inputs["input_ids"], max_length=200, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True)
        summary = summarizer_tokenizer.decode(summary_ids[0], skip_special_tokens=True)
        summaries.append(summary)

    return summaries

def translate_text(text):
    inputs = translator_tokenizer(text, return_tensors="pt")
    translated_ids = translator_model.generate(inputs["input_ids"])
    translation = translator_tokenizer.decode(translated_ids[0], skip_special_tokens=True)
    return translation

def translate_and_combine_summaries(summaries):
    translated_summaries = []
    for summary in summaries:
        translation = translate_text(summary)
        translated_summaries.append(translation)

    combined_translation = "\n".join(translated_summaries)
    return combined_translation

@app.post("/summarize_and_translate")
async def summarize_and_translate(input: TextInput):
    if not input.text:
        raise HTTPException(status_code=400, detail="No text provided")

    summaries = summarize_long_text(input.text)
    combined_translation = translate_and_combine_summaries(summaries)

    return {"translation": combined_translation}